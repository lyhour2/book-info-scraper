{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping using lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.html as web\n",
    "from lxml.etree import XPath\n",
    "import math\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseUrl=\"http://books.toscrape.com/\"\n",
    "bookUrl=baseUrl+\"catalogue/category/books/childrens_11/index.html\"\n",
    "pageUrl=baseUrl+\"catalogue/category/books/childrens_11/page-\" #page-1,pag\n",
    "columns=['title','price','stock','imageUrl','rating','url'] #for CSV head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empty dataSet and default page values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet=[]\n",
    "page=1\n",
    "totalPages=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dataSet to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeto_csv(data,filename,columns):\n",
    "     with open(filename,'w+',newline='',encoding=\"UTF-8\") as file:\n",
    "          writer = csv.DictWriter(file,fieldnames=columns)\n",
    "          writer.writeheader()\n",
    "          writer = csv.writer(file)\n",
    "          for element in data:\n",
    "               writer.writerows([element])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TotalPages found: 1\n",
      "Processing Page 1 from 1\n",
      "Rows in Dataset: 20\n",
      "Total items collected: 20\n"
     ]
    }
   ],
   "source": [
    "# Web scraping loop\n",
    "while page <= totalPages:\n",
    "    source = web.parse(pageUrl + str(page) + \".html\").getroot()  # Read and parse the page\n",
    "\n",
    "    # Pagination handling\n",
    "    if page == 1:\n",
    "        perpageArticles = source.xpath(\"//form[@class='form-horizontal']//input[@name='perpage']/@value\")\n",
    "        totalArticles = source.xpath(\"//form[@class='form-horizontal']//input[@name='total']/@value\")\n",
    "\n",
    "        if perpageArticles and totalArticles:\n",
    "            totalPages = math.ceil(int(totalArticles[0]) / int(perpageArticles[0]))\n",
    "\n",
    "        print(\"TotalPages found:\", totalPages)\n",
    "\n",
    "    print(f\"Processing Page {page} from {totalPages}\")\n",
    "\n",
    "    # Paths for individual elements\n",
    "    articles = source.xpath(\"//ol[contains(@class,'row')]/li[position()>0]\")\n",
    "    titlePath = \".//article[contains(@class,'product_pod')]/h3/a/@title\"\n",
    "    linkPath = \".//article[contains(@class,'product_pod')]/h3/a/@href\"\n",
    "    pricePath = \".//article/div[2]/p[contains(@class,'price_color')]/text()\"\n",
    "    stockPath = \".//article/div[2]/p[contains(@class,'availability')]/text()\"\n",
    "    imagePath = \".//article/div[1][contains(@class,'image_container')]/img/@src\"\n",
    "    ratingPath = \".//article/p[contains(@class,'star-rating')]/@class\"\n",
    "\n",
    "    # Iterate through all articles\n",
    "    for row in articles:\n",
    "        title = row.xpath(titlePath)[0].strip() if row.xpath(titlePath) else \"\"\n",
    "        link = row.xpath(linkPath)[0].replace('../../../', baseUrl + 'catalogue/') if row.xpath(linkPath) else \"\"\n",
    "        price = row.xpath(pricePath)[0] if row.xpath(pricePath) else \"\"\n",
    "        availability = row.xpath(stockPath)[0].strip() if row.xpath(stockPath) else \"\"\n",
    "        image = row.xpath(imagePath)[0].replace('../../../../', baseUrl).strip() if row.xpath(imagePath) else \"\"\n",
    "        rating = row.xpath(ratingPath)[0].replace('star-rating', '').strip() if row.xpath(ratingPath) else \"\"\n",
    "\n",
    "        # Add to dataset if title is not missing\n",
    "        if title:\n",
    "            dataSet.append([title, price, availability, image, rating, link])\n",
    "\n",
    "    print(\"Rows in Dataset:\", len(dataSet))\n",
    "    page += 1  # Increment page for loop\n",
    "\n",
    "# Print total number of elements collected\n",
    "print(f\"Total items collected: {len(dataSet)}\")\n",
    "\n",
    "# Save the dataset to a CSV file\n",
    "writeto_csv(dataSet, 'books.csv', columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
